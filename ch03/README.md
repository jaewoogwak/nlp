# Word2Vec

Word2Vec은 단어들간의 의미와 관계를 포착하는 기술이다.

Word2Vec을 사용하면 단어가 가지고 있는 의미들을 가지고 연산하는 것처럼 느낄 수 있다.

고양이 + 애교 = 강아지
한국 - 서울 + 도쿄 = 일본
박찬호 - 야구 + 축구 = 호나우두

실제로는 각 단어 벡터가 단어 간 유사도를 반영한 값을 가지고 있기 때문이다.

기존에는 원-핫 벡터를 사용했는데, 이 경우 단어 간 관계를 파악할 수 없다.

```python
[0,0,1,0,0]
[0,1,0,0,0]
```

Word2Vec에는 두 가지 방식이 있다.

-   CBOW(Continuous Bag of Words)
-   Skip-Gram

## CBOW

CBOW는 주변 단어들로 중심 단어를 예측 방법이다.

예를 들면, "The fat cat sat on the mat" 문장에서 The, fat과 sat, on으로 중심 단어 cat을 예측하는 것이다.

## Skip-Gram

Skip-Gram은 반대로 중심 단어 하나로 주변 단어들을 예측하는 방법이다.

cat 단어 하나로 The, fat, sat, on을 예측한다.

Skip-Gram이 CBOW보다 더 뛰어난 표현을 보이는 것으로 알려져 있다.

## Example

Word2Vec에 여러 인자를 넘겨 호출하면 학습이 진행된다.

```python
from gensim.models import Word2Vec

CBOW_W2V = Word2Vec(sentences = word2vec_train_datas,
                    vector_size = 32,
                    window = 5,
                    min_count = 1,
                    workers = 4,
                    sg = 0
                    )
# vector_size = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.
# window = 컨텍스트 윈도우 크기
# min_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)
# workers = 학습을 위한 프로세스 수
# sg = 0은 CBOW, 1은 Skip-gram.
```

아래와 같이 유사한 단어를 확인할 수 있다.

```python
CBOW_W2V.wv.most_similar("^^")
```

> [('~^^', 0.9216418266296387),
> > ('강추', 0.8709923028945923),
> > ('~', 0.8225168585777283),
> > ('부캡', 0.8158915638923645),
> > ('즐겁다', 0.8098419308662415),
> > ('화이팅', 0.8071112036705017),
> > ('추하다', 0.8034341931343079),
> > ('♡', 0.8012511730194092),
> > ('추워지다', 0.7990585565567017),
> > ('에요', 0.7978767156600952)]

아래와 같이 벡터 연산도 가능하다.

```python
print(CBOW_W2V.wv.most_similar(CBOW_W2V.wv["공포영화"]-CBOW_W2V.wv["공포"]+CBOW_W2V.wv["액션"]))
```

> [('액션', 0.8503912687301636), ('액션영화', 0.8195174336433411), ('견자단', 0.7563360333442688), ('상급', 0.742682933807373), ('좀비', 0.7423502802848816), ('공포물', 0.7251108884811401), ('촣', 0.7216518521308899), ('이연걸', 0.7163146734237671), ('성룡', 0.7064222097396851), ('대환영', 0.7011522650718689)]
